---
title: "**AdvStDaAn, Worksheet, Week 1**"
author: "Micheal Lappert"
date: '31.03.2022'
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: 3
  '': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      out.width = '25%')
source('Specific_R_functions/RFn_Plot-lmSim.R')
```

## Exercise 1
#### Dataset loading and sanity check:

```{r}
path <- file.path('Datasets', 'Softdrink.dat')
df <- read.table(path, header=TRUE)

summary(df)
head(df)
tail(df)
```
Data looks just fine.

### Exercise 1.a)
```{r}
mod1.1 <- lm(Time ~ volume, data = df)
summary(mod1.1)

```
The model looks fine:
- Volume is significant on the 5% niveau and the R-squared has a score of 0.93.

We have to do a residual and sensitivity analysis with stochastic simulation
to investigate the correctness of the model.

```{r}
plot(mod1.1)
plot.lmSim(mod1.1, SEED = 1)
```

#### Interpretation:
1.  Tukey-Anscombe plot: Shows outlier with index i=9 which affects the smoother. In the simulation it is visible that the original curve is extreme.\
=> The expectation of the residuals cannot be constant.
2.  Q-Q plot: In the lower as well as in the higher part of the plot some points differ from the straigt line. Most of them are within the stochastic fluctuation except i=9.\
=> The assumption of normal distributed residuals seems violated.
3.  Scale-location plot: Shows a clear upwards trend. In the simulation it is visible that the original curve is extreme.\
=> The variance of the residuals is not constant.
4.  Residuals vs. Leverage: Observations i = 9 & 22 have Cooke's Distance > 1 and are therefore too influentious. Both observations have addationally too much leverage.\
=> Residuals are not normally distributed.

*CONCLUSION*: The fit is not satisfactory. Trying transformations of response and explanatory variable. Since the noncanstant variance seems to be the most severe problem, log-transformations might help.
               
### Exercise 1.b)
#### Tukey's first-aid transformations:

```{r}
df$lVolume <- log(df$volume)
df$lTime <- log(df$Time)

head(df)
```
Model with transformed variables:
```{r}
mod1.2 <- lm(lTime ~ lVolume, data = df)
summary(mod1.2)
```
```{r}
plot(mod1.2)
plot.lmSim(mod1.2, SEED = 1)
```

#### Interpretation:
1.  Tukey-Anscombe plot: The smoother shows a somewhat strange banana form with the low in the middle which is outside the stochastic fluctuation.\
=> The assumption of constant expactaion is therefore violated.
2.  Q-Q plot: The data scatters nicely around the straight line and seems to be within the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors seems not violated.
3.  Scale-location plot: The smoother shows a slightly decreasing trend but seems to be ok and lies within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: All observations have Cook's Distance <1 and therewith no too influential points.\
=> No too influential (dangerous) observations

*CONCLUSION*: The model does still not fit adequately the data, although it is much better than the one before.

An alternative transformation for volume could be the square-root transformation. So let's try out:
```{r}
df$sVolume <- sqrt(df$volume)

mod1.3 <- lm(Time ~ sVolume, data = df)
summary(mod1.3)
```
The $R^{2}$ is with 0.8649 higher than before.

Residual and Sensitivity Analysis:
```{r}
plot(mod1.3)
plot.lmSim(mod1.3, SEED = 1)
```
#### Interpretation:
1.  Tukey-Anscombe plot: The smoother shows still a somewhat strange banana form with the low in the middle but like that it is inside the stochastic fluctuation.\
=> The assumption of constant expactation is not violated.
2.  Q-Q plot: The data scatters nicely around the straight line (except i=9) and seems to be within the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors seems not violated.
3.  Scale-location plot: The smoother looks ok and lies within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: All observations have Cook's Distance <1 and therewith no too influential points.\
=> No too influential (dangerous) observations

*CONCLUSION*: The model does still fit adequately the data.

### Exercise 1.c)
The fitte model in 1.b) is:

$$Time_i = exp(\beta_0) + exp(\beta_1) * sqrt(volume_i) + exp(E_i)$$
with $$\mu = 0$$ $$\sigma = \sigma$$

### Exercise 1.d)
Extending the model adequately with the second explanatory variable 'distance':
```{r}
df$lDistance <- log(df$distance)
mod1.4 <- lm(lTime ~ sVolume + lDistance, data = df)
summary(mod1.4)
```
The $R^{2}$ increases to 0.9443 which is a really good fit. Lets check the model:
```{r}
plot(mod1.4)
plot.lmSim(mod1.4, SEED = 1)
```

#### Interpretation:
1.  Tukey-Anscombe plot: The smoother shows a really nice almost straight line which is within the stochastic fluctuation.\
=> The assumption of constant expactation is not violated.
2.  Q-Q plot: The data scatters nicely around the straight line and seems to be within the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors seems not violated.
3.  Scale-location plot: The smoother shows a almost straight line and is within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: All observations have Cook's Distance <1 and therewith no too influential points. Some are leverage point with leverage > $2*3/25=0.24$ (25 examples in dataset)\
=> No too influential (dangerous) observations

*CONCLUSION*: The model does fit adequately the data.

## Exercise 2
#### Loading and Checking the data
```{r}
path <- file.path('Datasets', 'Windmill.dat')
df <- read.table(path, header = TRUE)

summary(df)
dim(df)
head(df)
tail(df)
```

### Exercise 2.a)
Start with fitting an ordinary regression model:
```{r}
mod2.1 <- lm(DC.output ~ velocity, data = df)
summary(mod2.1)
```
The model seems to fit not too bad. Lets check this:
```{r}
plot(mod2.1)
plot.lmSim(mod2.1, SEED = 1)
```

#### Interpretation:
1.  Tukey-Anscombe plot: The smoother shows a banana form which is outside the stochastic fluctuation.\
=> The assumption of constant expactation is violated.
2.  Q-Q plot: The data does not scatter nicely around the straight line and but seems to be within the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors seems not violated.
3.  Scale-location plot: The smoother shows a almost straight line and is within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: All observations have Cook's Distance <1 and therewith no too influential points. There are also no leverage points with leverage > $2*2/25=$ `r 2*2/25` (25 examples in dataset, 2 variables in model)\
=> No too influential (dangerous) observations

*CONCLUSION*: The model does not fit adequately the data. Maybe some transfromations would help to remedy the inadequacy. -> Exercise 2.b)

### Exercise 2.b)
```{r}
df$lVelocity <- log(df$velocity)
df$lDC.output <- log(df$DC.output)

mod2.2 <- lm(lDC.output ~ lVelocity, data = df)
summary(mod2.2)
```
Variable seems to be significant and $R^{2} is with 0.7371 not too bad. But we could surely do better with some adjustments. Lets check the model:
```{r}
plot(mod2.2)
plot.lmSim(mod2.2, SEED = 1)
```
#### Interpretation:
1.  Tukey-Anscombe plot: The smoother still shows a banana form which is outside the stochastic fluctuation.\
=> The assumption of constant expactation is violated.
2.  Q-Q plot: The data does better scatter around the straight line (except outlier i=25) but is outside the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors is violated.
3.  Scale-location plot: The smoother shows a wavy line and is within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: Observation i=25 has Cook's Distance >1 and therewith is too influential. But there are no leverage points with leverage > $2*2/25=$ `r 2*2/25` (25 examples in dataset, 2 variables in model)\
=> i=25 is too influential observations

*CONCLUSION*: The model does not fit adequately the data. Maybe some transfromations would help to remedy the inadequacy. -> Exercise 2.c)

### Exercise 2.c)
```{r}
df$tVelocity <- 1/df$velocity

mod2.3 <- lm(DC.output ~ tVelocity, data = df)
summary(mod2.3)
```
This model seems to fit the data way better. The transformation from theroy seems pretty adequat. Lets check the model:
```{r}
par(mfrow = c(2,4))
plot(mod2.3)
plot.lmSim(mod2.3, SEED = 1)
```

#### Interpretation:
1.  Tukey-Anscombe plot: The smoother shows now no banana form anymore and is within the stochastic fluctuation.\
=> The assumption of constant expactation of the errors is not violated.
2.  Q-Q plot: The data does scatter around the straight line and is within the stochastic fluctuation.\
=> The assumption of Gaussian distributed errors is not violated.
3.  Scale-location plot: The smoother shows a little wavy line but is within the stochastic fluctuation.\
=> There is no evidence against the assumption of constant variance of the residuals.
4.  Residuals vs. Leverage: No Observation has Cook's Distance >1. And there are no leverage points with leverage > $2*2/25=$ `r 2*2/25` (25 examples in dataset, 2 variables in model)\
=> i=25 is too influential observations

*CONCLUSION*: The model does fit adequately the data. The transformation of the velocity variable did indeed remedy the model inadequaties.

### Exercise 2.d)
Plotting the model for the interpretation of the parameters $\beta_0$ and $\beta_1$.
```{r, out.width='100%'}
par(mfrow=c(1,1))
plot(DC.output ~ velocity, data = df,
     ylim = range(df$DC.output, coef(mod2.3)[1], 0)) 
# -> The range() takes the larger value of DC.output and the intercept
abline(h = coef(mod2.3)[1], col = "red")

# predicted DC.output
range(df$tVelocity)
df2 <- data.frame(tVelocity = seq(0.043, 0.185, length = 50))
lines(1/df2$tVelocity, predict(mod2.3, newdata = df2))

# How much wind is needed at least?
(h.minW <- -coef(mod2.3)[2]/coef(mod2.3)[1])  ## = 5.208521 m/s
abline(v=h.minW, col=5, lty=6)
abline(h=0)
```

## Exercise 3