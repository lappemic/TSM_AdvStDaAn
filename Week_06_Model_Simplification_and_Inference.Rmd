---
title: "**AdvStDaAn, Worksheet, Week 6**"
author: "Michael Lappert"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  pdf_document:
    toc: yes
    toc_depth: 3
  html_document:
    toc: yes
    toc_depth: 3
  '': default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
source('Specific_R_functions/RFn_Plot-lmSim.R')
library(purrr)
library(tidyr)
library(ggplot2)
```

## Exercise 1
#### Dataset loading and sanity check:

```{r, out.width='100%'}
path <- file.path('Datasets', 'baby.dat')
df <- read.table(path, header=TRUE)

summary(df)
str(df)
head(df)
tail(df)
plot(df)

df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

### Exercise 1.a)
```{r}
glm1.1 <- glm(Survival ~ ., family = binomial, data = df)
summary(glm1.1)
```

On a first sight, just Weight and Age seem to be sifnificant on the 5% significance level. To test this hypothesis, one must perform a statistical test:

Since (from the summary output)
```{r}
1-pchisq(319.28-236.14, df=246-241) # Compare slide 12&13 from w6
```

is smaller than the significant level of 5%, we cannot drop all explanatory variables. At least one of them is significant.

Or without plugging in the numbers explicitly (same as above in other synthax):
```{r}
(h <- summary(glm1.1)$null.deviance - summary(glm1.1)$deviance)
1 - pchisq(h, 246-241)
```


This test is identical to
```{r}
glm1.2 <- glm(Survival ~ 1, family=binomial, data = df)
anova(glm1.1, glm1.2, test="Chisq")
```

Where we also conclude that since the p-value of 2.2e-16 is << than the significance level of 0.05 to reject the null hypothesis and assume that the first (full) model describes the data more adequately than the second (empty) one and therefore at least one variable is of significance.

### Questions1.a)
- How do we already now, that the response is Bernoulli distributed?\

### Exercise 1.b)
Performing a stepwise variable selection.
```{r}
glm.step1.1 <- step(glm1.1, scope = list(upper =~ .,
                                         lower =~ 1),
                    direction = 'both')
summary(glm.step1.1)
```

The variables Agpar5 and pH got dropped.

### Exercise 1.c)
Fitting a model with the explanatory variables Weight and Age and comparing it with anova at the 5% significance level.
```{r}
glm1.3 <- glm(Survival ~ Weight + Age, family = binomial, data = df)
summary(glm1.3)
anova(glm1.1, glm1.3, test = 'Chisq')
```

Since the p-value is 0.29 and therefore bigger than the significance level of 5% the null Hypothesis can not be rejected concluding that both models describe the data in the same adequacy. Therefore one can conclude that the model 'Survival ~ Weight + Age' describes the data statistically equally well as the full one.

## Exersice 2
#### Dataset loading and sanity check:

```{r, out.width='100%'}
path <- file.path('Datasets', 'twomodes.dat')
df <- read.table(path, header=TRUE)

summary(df)
str(df)
head(df)
tail(df)
plot(df)

df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

### Exercise 2.a)
- Response: Failures
- Distribution: Poisson
- Explanatory variables: mode1 & mode2
- Link function: ~~log~~ -> rather identity, because one rather wants a direct influence of the operating time on the failure rate in each mode. This choice is supported by the fact that both operating times are positive explanatory variables, and thus, with positive parameter values, the linear predictor is also positive. Therefore, the link "identity" guarantees a positive failure rate.-- But the log link is not excluded by these arguments!

### Question 2.a)
What is a good suggestion of the procedure to finde the right model parameters like distribution and especially link function?

### Exercise 2.b)
Fit the suggested model in a):
```{r}
glm2.1 <- glm(Failures ~ ., family = poisson(link = 'identity'), data = df)
summary(glm2.1)
```

Since the coefficients have positive signs and therefore are positive linear predictors the signs are correct.

### Exercise 2.c)
Another model that can be considered, as stated in the worksheet, uses neither an intercept nor the explanatory variable *mode2*; that is,
*Failures ~ -1 + mode1*

What are the pros and cons of this reduced model?

- Pros
  + in practical application, it has been repeatedly shown that the intercept collects systematic errors in both the response and the explanatory variables, which would be avoided this way
- Cons
  + The intercept must be interpreted somehow, but is not included in this model
  
Fitting the suggested model and comparing it to the original one fitted in b):
```{r}
glm2.2 <- glm(Failures ~ -1 + Mode1, family = poisson(link = 'identity'), data = df)
summary(glm2.2)
```

### Question 2.c)
Is this explanation right, why the Null deviance is inf?
The null deviance is Inf (infinite) because it describes the residuals with only the intercept and because there is no intercept in this model, the model has no residuals there.

```{r}
anova(glm2.1, glm2.2, test = 'Chisq')
```

Since the p-value of the newly suggested (reduced) model is 0.06972 and therefore > than the significance level of 5% we can not reject the null Hypothesis and conclude, that both models describe the model statistically equally well.

## Exercise 3
#### Dataset loading and sanity check:

```{r, out.width='100%'}
path <- file.path('Datasets', 'nambeware.txt')
df <- read.table(path, header=TRUE)

summary(df)
str(df)
head(df)
tail(df)
plot(df)

df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()
```

### Exercise 3.a)
Testing the if the model using the linear predictor 'Diam * Type' describe the data of Nambeware better than the model with the linear predictor 'Diam + Type':
```{r}
glm3.1 <- glm(Time ~ Diam * Type, family = Gamma(link = log), data = df)
glm3.2 <- glm(Time ~ Diam + Type, family = Gamma(link = log), data = df)

anova(glm3.1, glm3.2, test = 'Chisq')
```

Since the p-value of the second model is > than the significance level of 5% we can not reject the null Hypothesis and conclude that both models describe the data equally well and use therefore the reduced model (glm3.2).

## Exercise 4
#### Dataset loading and sanity check:

```{r, out.width='100%'}
path <- file.path('Datasets', 'O-rings.dat')
df <- read.table(path, header=TRUE)

summary(df)
str(df)
head(df)
tail(df)
plot(df)

df %>%
  keep(is.numeric) %>% 
  gather() %>% 
  ggplot(aes(value)) +
    facet_wrap(~ key, scales = "free") +
    geom_histogram()

par(mfrow=c(1,1))
sunflowerplot(df$Temp, df$Fails/df$m, number=df$m, las=1)
```

### Exercise 4.a)
- Response: Fails
- Distribution: binomial with expectation mu=pi_i and size=m_i
- Explanatory Variables: m, Temp, Pres
- Link function: canonical link because there is none mentioned explicitly. But alternative:     + complementary log-log link because the topic is material fatigue
- Model: 
    + glm(cbind(Failures, m-Failures) ~ Temp + Pres, family = binomial(link = logit), data = df)
    
### Question 4.a)
How do we knoe that the link function is logit? With just stated to use the canonical link, as in the solutions, to me is not clear why logit is the reasonable choice.

### Exercise 4.b)
Fit the model proposed in a)
```{r}
glm4.1 <- glm(cbind(Fails, m-Fails) ~ Temp + Pres, family = binomial(link = logit),
              data = df)
summary(glm4.1)
```

The relevance of the pressure at which safety testing for field join leaks was performed to the failure process was unclear. The p-value of Pres is > than the significance level of 5% and we conclude that it is not significant to describe the data (Wald statistics). But lets compare another model without the pressure to the first model.

```{r}
glm4.2 <- glm(cbind(Fails, m-Fails) ~ Temp, family = binomial(link = logit), data = df)
anova(glm4.1, glm4.2, test = 'Chisq')
```

Since the p-value of 0.2174 is > than the significance level of 5% we can not reject the null hypothesis and conclude that both models describe the data statistically equally well and can use in practice the reduced one (based on the preferred deviance statistics here). However, we do not know what risk of using the reduced model is (i.e. the probability of type II error)

Testing using the confidence intervals
Wald:
```{r}
(h <- summary(glm4.1)$coefficients)
h[3,1] + c(-1,1)*qnorm(0.975) * h[3,2]  ##  -0.0053  0.0200
```

The 95% confidence interval  covers the null hypothesis 'beta1 = 0' (0 is between -0.0053 and 0.02). Hence we have no evidence against the null hypothesis.

Deviance (via profiling):
```{r}
confint(glm4.1)
```
This 95% confidence interval ([-0.004030283,  0.02272544]) covers the null hypothesis 'beta1 = 0' as well. So we obtain the same conclusion.

### Exercise 4.c)
Predict the probability that an O-ring will leak at the expected tempreature of 31°F at launch.
```{r}
preds1 <- predict(glm4.2, type = 'response', 
                   se.fit = TRUE, newdata = data.frame(Temp = 31))
preds1$fit
preds1$fit + c(-1, 1) * qnorm(0.975) * preds1$se.fit
```

This leads to a very large confidence interval and lies not within the support of [0,1]. Therefore we try another approach:
```{r}
preds2 <- predict(glm4.2, type="link",
                  se.fit = TRUE, newdata=data.frame(Temp=31))
(preds2adj <- preds2$fit + c(-1,1) * qnorm(0.975) * preds2$se.fit)
1/(1 + exp(-preds2adj))
```

The 95% confidence interval covers almost the whole support except the area to 0. But the probability that an o-ring will fail may be close to 1! There is not much confidence that the o-ring will sustain.

### Exercise 4.d)
Repeating the above analysis with just those observatinos in which at least one failure occured:
```{r}
df2 <- df[(df$Fails != 0), ]
str(df2)
summary(df2)
nrow(df) - nrow(df2)
nrow(df2)

glm4.3 <- glm(cbind(Fails, m-Fails) ~ Pres + Temp,
              family = binomial(link = logit), data = df2)
glm4.4 <- glm(cbind(Fails, m-Fails) ~ Temp,
              family = binomial(link = logit), data = df2)
glm4.5 <- glm(cbind(Fails, m-Fails) ~ 1,
              family = binomial(link = logit), data = df2)

anova(glm4.3, glm4.4, test = 'Chisq')
```

The two model describe the data statistically equally well, so we use the reduced one.
```{r}
anova(glm4.4, glm4.5, test = 'Chisq')
```

Also this models describe the data statistically equally well, so we would again use the reduced one (with just the intercept...). Justify this by a variable selection with step():
```{r}
step(glm4.3)
```

The best model is again the one with just the intercept. So we could start at any temperature.

95% confidence interval for the probability of a defect o-ring:
```{r}
preds3 <- predict(glm4.5, newdata=data.frame(Temp=31), type="link", se=T)
family(glm4.5)$linkinv(preds3$fit + c(-1,1) * qnorm(0.975) * preds3$se.fit)

```

To compare with CI based on glm4.2
```{r}
preds4 <- predict(glm4.2, newdata=data.frame(Temp=31), type="link", se=T)
family(glm4.2)$linkinv(preds4$fit + c(-1,1) * qnorm(0.975) * preds4$se.fit)
```

this one is much wider than the one before.

(from solutions)\
Based on this "reduced" dataset, one could easily be convinced that temperature does not affect O-ring performance. Hence, based on this "reduced" dataset the conclusion which the scientists and engineers drew was correct.

But, when you conduct a statistical analysis on a sample of the available data, you can induce what in statistics is known as a sample selection problem. Running an analysis on less than the entire data set is not always a problem, but it can lead to mistaken conclusions depending on the question you are trying to answer.

Lessons learned:
1. Be very, very careful when predicting "out-of-sample" support.
2. Don't sample when all data points are available ... all launches, not just ones with O-ring distress.


### Exercise 4.e) (copied from solutions)
Display the data properly assuming just an eﬀect of temperature on the response and overlay the corresponding ﬁt using all data or the reduced dataset. In addtion, overlay both 95% conﬁdence intervals at a temperature of 31 ◦ F. What do you conclude from this?
```{r}
h.xlim <- c(30, max(df$Temp))
new.df <- data.frame(Temp=seq(h.xlim[1], h.xlim[2], length=50))
h.predGLM2 <- predict(glm4.2, newdata=new.df, type="response")
h.pred1GLM2 <- predict(glm4.4, newdata=new.df, type="response")

sunflowerplot(df$Temp, df$Fails/df$m, number=df$m, las=1,
              xlim=h.xlim, ylim=c(0,1))
## fit
lines(new.df$Temp, h.predGLM2, col="blue")
lines(new.df$Temp, h.pred1GLM2, col="red")
legend(x=50, y=1, legend=c("Fit using all data", "Fit using reduced dataset"),
       col=c("blue", "red"), lty=c(1,1))

## confidence intervals
h2 <- predict(glm4.2, newdata=data.frame(Temp=31), type="link", se=T)
h2.ci <- family(glm4.2)$linkinv(h2$fit + c(-1,1)*qnorm(0.975)*h2$se.fit)
h12 <- predict(glm4.4, newdata=data.frame(Temp=31), type="link", se=T)
h12.ci <- family(glm4.4)$linkinv(h12$fit + c(-1,1)*qnorm(0.975)*h12$se.fit)

lines(c(31,31), h2.ci, col="blue", lwd=2)
lines(c(31.3,31.3), h12.ci, col="red", lwd=2)

```

Both confidence intervals are huge indicating that there is a great uncertainty in the predicted probabilities independent of the applied fit.

